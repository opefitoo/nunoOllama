# LLM Provider Configuration
# Supported providers: ollama (local), deepseek, openai, anthropic
LLM_PROVIDER=ollama

# Model Selection
# Ollama (Local): llama3.1:8b (recommended), qwen2.5:14b, llama3.1:70b
# DeepSeek: deepseek-reasoner (recommended for constraint reasoning), deepseek-chat
# OpenAI: gpt-4-turbo, gpt-4, gpt-3.5-turbo
# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
LLM_MODEL=llama3.1:8b

# Ollama Configuration (for local LLM)
# Default: http://localhost:11434/v1 (when Ollama runs on host machine)
# Docker: http://host.docker.internal:11434/v1 (when AI orchestrator is in Docker)
# Docker Compose: http://ollama:11434/v1 (when both services in Docker)
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1

# API Keys (only needed for cloud providers, not for Ollama)
LLM_API_KEY=not_needed_for_ollama

# Optional: OpenAI specific key (if using openai provider)
# OPENAI_API_KEY=your_openai_key_here

# Optional: Anthropic specific key (if using anthropic provider)
# ANTHROPIC_API_KEY=your_anthropic_key_here
